{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNJjz8kK8ioVo7Z5j30qFr4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tlsgptj/emotional_BERT/blob/main/KOBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#크롤링\n",
        "#기쁨, 슬픔, 놀람, 분노, 공포, 혐오\n",
        "import json\n",
        "import csv\n",
        "\n",
        "with open('/content/감성대화말뭉치(최종데이터)_Training.json', 'r') as json_file:\n",
        "  data = json.load(json_file)\n",
        "\n",
        "with open('csv_file.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
        "  writer = csv.writer(csv_file)\n",
        "  header = data[0].keys()\n",
        "  writer.writerow(header)\n",
        "  for row in data:\n",
        "    writer.writerow(row.values())"
      ],
      "metadata": {
        "id": "Zx68RIw0yHNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mxnet # 코랩 환경이기 때문에 앞에 !를 붙여야 한다.\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "I3XjGr26nOZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, BertModel\n",
        "\n",
        "# Try loading the tokenizer using AutoTokenizer\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    print(\"Please check the model name and your internet connection.\")\n",
        "\n",
        "# Load the KoBERT model\n",
        "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "\n",
        "# ... rest of your code ...le_with_warmup"
      ],
      "metadata": {
        "id": "lnXM2h4HoOto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "# JSON 파일 로드\n",
        "with open('/content/감성대화말뭉치(최종데이터)_Training.json', 'r', encoding='utf-8') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "# HS01과 HS02 추출 및 합치기\n",
        "extracted_data = []\n",
        "for row in data:\n",
        "    talk_data = row.get('talk', {})  # 'talk' 데이터를 가져옴\n",
        "    content = talk_data.get('content', {})\n",
        "    hs01 = content.get('HS01', '')\n",
        "    hs02 = content.get('HS02', '')\n",
        "\n",
        "    # HS01과 HS02를 합치기\n",
        "    combined_hs = f\"{hs01} {hs02}\".strip()  # 문자열을 합치고 불필요한 공백 제거\n",
        "\n",
        "    if combined_hs:  # combined_hs가 비어있지 않으면 추가\n",
        "        extracted_data.append([combined_hs])\n",
        "\n",
        "# CSV 파일로 저장\n",
        "csv_file_path = 'extracted_hs_data.csv'\n",
        "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Combined_HS'])  # 컬럼명 작성\n",
        "    writer.writerows(extracted_data)\n",
        "\n",
        "print(f'CSV 파일이 {csv_file_path}에 저장되었습니다.')"
      ],
      "metadata": {
        "id": "QLnFCdATpbUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# talk에 있는 데이터만 추출해야함"
      ],
      "metadata": {
        "id": "3tXftmuxwUHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "emotional = pd.read_csv('/content/extracted_hs_data.csv')"
      ],
      "metadata": {
        "id": "02k08tMEw3z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotional.head(20)"
      ],
      "metadata": {
        "id": "jzBRgkH1y0RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, BertModel\n",
        "\n",
        "# Try loading the tokenizer using AutoTokenizer\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    print(\"Please check the model name and your internet connection.\")\n",
        "\n",
        "# Load the KoBERT model\n",
        "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "\n",
        "# ... rest of your code ...le_with_warmup"
      ],
      "metadata": {
        "id": "Oz3PonV5zQPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 과정\n",
        "# 1. 텍스트 데이터 정제\n",
        "# 2. 문서를 토큰으로 나누기\n",
        "# 3. 불용어 제거\n",
        "# 4. 모델을 적용할꺼임 KOBERT적용 등...\n",
        "# 5. TF-DIF로 알고리즘 비교\n",
        "# koBERT의 경우 벡터로 변환할 필요가 없다고 함 : KoBERT는 텍스트를 바로 토큰화한 뒤 임베딩 벡터로 변환하는 기능을 내장하고 있기 때문"
      ],
      "metadata": {
        "id": "DvVkTo8Bzqq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 과정 자세히\n",
        "# [영어]\n",
        "# 소문자 변환\n",
        "# 불필요한 공백 제거\n",
        "# 구두점 제거\n",
        "# 특수 문자 제거\n",
        "# 숫자 제거\n",
        "# 불용어 제거\n",
        "# 형태소 분석\n",
        "# 어간 추출\n",
        "# 표제어 추출\n",
        "# 철자 교정\n",
        "# 단어 토큰화\n",
        "# 벡터화\n",
        "## -> 여기서 한국어일 경우 1번은 생략하고 분석"
      ],
      "metadata": {
        "id": "XP0oE22I0NGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 구두점 제거\n",
        "import string\n",
        "import re\n",
        "emotional['Combined_HS'] = emotional['Combined_HS'].str.translate(str.maketrans('', '', string.punctuation))\n",
        "# 특수 문자 제거\n",
        "emotional['Combined_HS'] = emotional['Combined_HS'].apply(lambda x: re.sub(r\"[^a-zA-Z0-9가-힣\\s]\", \"\", x) if isinstance(x, str) else x)\n",
        "# 숫자 제거\n",
        "emotional['Combined_HS'] = emotional['Combined_HS'].apply(lambda x: re.sub(r'\\d+', '', x) if isinstance(x, str) else x)"
      ],
      "metadata": {
        "id": "dIHR3oZO0bjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# 한국어 불용어 추가\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(['많아서','받아','나의','년','싶지','친구와','하네','시간이','자기','같은데','살','벌써','직장에서','결혼을','돼서','신경','조금','근데','친구들과','어떤','있지','꼭','친구에게','대해','가고','회사에','함께','돈도','나보다','이젠','빨리','친구들은','부모님께','하니','들어서','일도','아주','나랑','결혼','되었어','됐어','노후','모든','학교','할까','나서','몇','건강이','자식들이','하지만','그동안','먼저','학교에서','나와','중이야','많은','병원에','그런데','기분이야','있을','건지','나이','친한','받았어','많아', '뭘','되고','마음이','자주','점점','무슨','위해','있을까','제대로','사는','그래도','일','직장','된','친구를','아무것도','먹고','것을','모두','아무도','하나', '아빠가','참','없는데','말이야','때마다','될','어제','공부를','친구','해도','되는','전에', '하면','보면', '성적이','큰','또', '않고', '같은','것이', '없이','사람들이','얼마', '이번', '회사', '집에', '앞으로', '걸까','아직', '하지', '텐데', '매일', '부모님이', '자신이', '수가', '나에게', '생각이', '않아서', '이번에', '회사에서', '건', '그런', '그래서', '없고', '줄', '그래서', '나만', '없는', '했어', '이다', '하다', '때', '돼', '싶은데', '없어서', '날', '나이가', '같이', '난다', '것', '같아', '나는', '내가', '다', '계속', '나', '내', '더', '남편이', '아내가', '오늘', '난', '정말', '많이', '요즘', '이제', '친구가', '나도', '자꾸', '해', '좀', '나를', '했는데', '응', '그', '갑자기', '거야', '엄마가', '할수', '항상', '잘', '있는데', '어떻게 해야', '때문에', '아들이', '이제는', '친구들이', '딸이', '그렇게', '돈을', '다들', '다시', '않아', '그냥', '있어', '진짜', '돈', '너무', '안', '못', '계', '수', '하고', '없어', '할', '어떻게', '봐', '게', '하는', '우리', '왜', '거', '해서', '한', '있는', '들어', '싶어', '해야', '같아서', '하는데', '일을', '다른', '할지', '걸', '이', '이렇게', '일이', '말을', '것도', '몸이', '나한테', '지금', '있어서', '돈이', '보니', '이런', '사람이'])\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  if isinstance(text, str):\n",
        "    tokens = text.split()\n",
        "    result = [word for word in tokens if not word in stop_words]\n",
        "    return \" \".join(result)\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "emotional['Combined_HS'] = emotional['Combined_HS'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "vK4K6qF66bY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y fonts-nanum"
      ],
      "metadata": {
        "id": "TPFtVoRhOqkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all text in the 'Combined_HS' column into a single string\n",
        "text = ' '.join(emotional['Combined_HS'].astype(str).tolist())\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=800, background_color='white', font_path='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf').generate(text)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(8, 8), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0dbCUkChOdU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotional.head(20)"
      ],
      "metadata": {
        "id": "OOJ9nGFr0iKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "emotion_words = {\n",
        "    \"기쁨\": [\"만족\",\"자랑\",\"기뻐\", \"좋은\", \"좋아\", \"좋아하는\", \"기분이 좋아\", \"다행\", \"원하던\", \"좋았거든\", \"웃게\", \"기뻤고\", \"만족하고\", \"성공했어\", \"합격했어\", \"기뻤어.\", \"가벼워졌어\", \"행복해\", \"편해진\"],\n",
        "    \"슬픔\": [\"안쓰러워\",\"낙담이\",\"슬퍼\", \"속상해\", \"힘들어서\", \"미안해\", \"후회돼\", \"억울해\", \"괴로워\", \"외로워\", \"힘들고\", \"한심해\", \"눈물이\", \"섭섭해\", \"취소\", \"버려진\", \"망했어\", \"실망스러워\", \"싸웠어\", \"속상한\", \"불쌍해\", \"상처받았어\", \"힘들어\", \"서운해\", \"미워\", \"서글퍼\", \"슬프군\", \"슬프지만\", \"섭섭할\", \"슬픈\", \"서운하네\", \"슬프다\", \"슬프기도\", \"슬프네.\", \"슬프고\", \"씁쓸해\", \"서러워\"],\n",
        "    \"걱정\": [\"당황\", \"혼란\", \"두려워\", \"모르겠어\", \"초조해\", \"걱정이야\", \"어떡하지\", \"걱정이\", \"불안해\", \"고민이\", \"당황스럽더라고\", \"걱정\", \"마비\", \"이래도\", \"불안했어\"],\n",
        "    \"분노\": [\"역겨울\",\"환멸이\",\"원망해도\",\"얄미워\",\"짜증나\", \"화가\", \"짜증\", \"짜증이\", \"스트레스\", \"답답해\", \"분하고\", \"배신감\", \"버린 거나\", \"의심스러워\"],\n",
        "    \"공포\": [\"무섭네.\",\"두려워\", \"두려움\", \"무서워\", \"무서움\", \"힘들어\"],\n",
        "    \"우울\": [\"힘들고\", \"외로워\", \"외롭고\", \"어려워졌어\", \"버겁고\", \"우울해\", \"우울하군\", \"고독하다.\", \"쓸쓸\"],\n",
        "    \"부끄러움\": [\"당황스러웠지\",\"창피하고\",\"부끄러워\", \"창피해\", \"실수\", \"당혹스러웠어\", \"당황했어\", \"민망해\", \"망신을\", \"당황스러워\"],\n",
        "    \"감사\": [\"감사해\", \"감사했지\"],\n",
        "    \"놀람\": [\"놀랐어\"],\n",
        "    \"후회\" :[\"후회\"],\n",
        "    \"기타\": []\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# 감정 레이블\n",
        "emotion_labels = {\n",
        "    \"기쁨\": 0,\n",
        "    \"슬픔\": 1,\n",
        "    \"걱정\": 2,\n",
        "    \"분노\": 3,\n",
        "    \"공포\": 4,\n",
        "    \"우울\": 5,\n",
        "    \"부끄러움\": 6,\n",
        "    \"감사\":7,\n",
        "    \"놀람\":8,\n",
        "    \"후회\":9,\n",
        "    \"기타\":10\n",
        "}\n",
        "\n",
        "# 텍스트에서 문장 부호 제거 함수\n",
        "def clean_text(text):\n",
        "    # 정규 표현식을 사용하여 문자와 숫자가 아닌 것을 공백으로 치환\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# 텍스트에서 감정 레이블 추출 함수 (형태소 분석 추가)\n",
        "def label_emotion(text):\n",
        "    text = clean_text(text)  # 문장 부호 제거\n",
        "    tokens = okt.morphs(text)  # 형태소 분석을 통해 텍스트를 토큰화\n",
        "    emotion_count = {emotion: 0 for emotion in emotion_words}  # 감정별 카운트를 위한 딕셔너리\n",
        "\n",
        "    for emotion, words in emotion_words.items():\n",
        "        for word in words:\n",
        "            if word in tokens:\n",
        "                emotion_count[emotion] += 1  # 감정 단어가 발견되면 카운트 증가\n",
        "\n",
        "    max_emotion = max(emotion_count, key=emotion_count.get)  # 가장 많이 등장한 감정 선택\n",
        "    if emotion_count[max_emotion] > 0:\n",
        "        return emotion_labels[max_emotion]\n",
        "    return emotion_labels[\"기타\"]  # 감정 단어가 없을 경우 \"기타\"로 처리\n",
        "\n",
        "# CSV 파일에서 텍스트 읽고 라벨링한 후 저장[[/0]]\n",
        "def label_csv(input_file, output_file):\n",
        "    with open(input_file, mode='r', encoding='utf-8') as infile, open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
        "        reader = csv.reader(infile)\n",
        "        writer = csv.writer(outfile)\n",
        "\n",
        "        # 헤더 작성\n",
        "        header = next(reader)  # 기존 CSV의 헤더를 가져옴\n",
        "        header.append('Emotion_Label')  # 새로운 열 추가\n",
        "        writer.writerow(header)\n",
        "\n",
        "        # 각 행에 대해 감정 레이블 추가\n",
        "        for row in reader:\n",
        "            text = row[0]  # 텍스트가 들어 있는 열 (필요 시 인덱스 조정)\n",
        "            emotion_label = label_emotion(text)\n",
        "            row.append(emotion_label)\n",
        "            writer.writerow(row)\n",
        "\n",
        "# 10번 라벨(기타)만 추출하는 함수\n",
        "def extract_label_10(input_file, output_file):\n",
        "    with open(input_file, mode='r', encoding='utf-8') as infile, open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
        "        reader = csv.reader(infile)\n",
        "        writer = csv.writer(outfile)\n",
        "\n",
        "        # 헤더 작성\n",
        "        header = next(reader)\n",
        "        writer.writerow(header)\n",
        "\n",
        "        # 10번 레이블(기타)인 행만 추출하여 작성\n",
        "        for row in reader:\n",
        "            if row[-1] == '10':  # 마지막 열이 '10'(기타 레이블)인 경우\n",
        "                writer.writerow(row)\n",
        "\n",
        "# 10번 라벨(기타)만 추출하고 개수 카운트하는 함수\n",
        "def extract_label_10(input_file, output_file):\n",
        "    count = 0  # 10번 라벨의 개수를 세기 위한 변수\n",
        "    with open(input_file, mode='r', encoding='utf-8') as infile, open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
        "        reader = csv.reader(infile)\n",
        "        writer = csv.writer(outfile)\n",
        "\n",
        "        # 헤더 작성\n",
        "        header = next(reader)\n",
        "        writer.writerow(header)\n",
        "\n",
        "        # 10번 레이블(기타)인 행만 추출하여 작성\n",
        "        for row in reader:\n",
        "            if row[-1] == '10':  # 마지막 열이 '10'(기타 레이블)인 경우\n",
        "                writer.writerow(row)\n",
        "                count += 1  # 10번 레이블 개수 증가\n",
        "\n",
        "    print(f\"10번 라벨(기타) 데이터는 총 {count}개 있습니다.\")\n",
        "    return count\n",
        "\n",
        "# CSV 파일 경로 설정\n",
        "input_csv = '/content/extracted_hs_data.csv'  # 감정 분석할 CSV 파일 경로\n",
        "output_csv = 'labeled_output.csv'  # 라벨링된 결과를 저장할 CSV 파일 경로\n",
        "\n",
        "# CSV 파일에 라벨링 적용\n",
        "label_csv(input_csv, output_csv)\n"
      ],
      "metadata": {
        "id": "yBkSViTuvya1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y fonts-nanum"
      ],
      "metadata": {
        "id": "-95D08ScPOCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "emotion = pd.read_csv('/content/label_10_output.csv')"
      ],
      "metadata": {
        "id": "LT002IJTPDFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion.tail(20)"
      ],
      "metadata": {
        "id": "j6kasB89lXPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 구두점 제거\n",
        "import string\n",
        "import re\n",
        "emotion['Combined_HS'] = emotion['Combined_HS'].str.translate(str.maketrans('', '', string.punctuation))\n",
        "# 특수 문자 제거\n",
        "emotion['Combined_HS'] = emotion['Combined_HS'].apply(lambda x: re.sub(r\"[^a-zA-Z0-9가-힣\\s]\", \"\", x) if isinstance(x, str) else x)\n",
        "# 숫자 제거\n",
        "emotion['Combined_HS'] = emotion['Combined_HS'].apply(lambda x: re.sub(r'\\d+', '', x) if isinstance(x, str) else x)"
      ],
      "metadata": {
        "id": "JP1CC3d_WYbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# 한국어 불용어 추가\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(['아무래도','그래','있었어','어려워','기분이','느껴져','고민이야','친구는','있었는데','좋겠어','죄책감이','싶은','보고','싫어','맞아','혼자','열심히','많아서','받아','나의','년','싶지','친구와','하네','시간이','자기','같은데','살','벌써','직장에서','결혼을','돼서','신경','조금','근데','친구들과','어떤','있지','꼭','친구에게','대해','가고','회사에','함께','돈도','나보다','이젠','빨리','친구들은','부모님께','하니','들어서','일도','아주','나랑','결혼','되었어','됐어','노후','모든','학교','할까','나서','몇','건강이','자식들이','하지만','그동안','먼저','학교에서','나와','중이야','많은','병원에','그런데','기분이야','있을','건지','나이','친한','받았어','많아', '뭘','되고','마음이','자주','점점','무슨','위해','있을까','제대로','사는','그래도','일','직장','된','친구를','아무것도','먹고','것을','모두','아무도','하나', '아빠가','참','없는데','말이야','때마다','될','어제','공부를','친구','해도','되는','전에', '하면','보면', '성적이','큰','또', '않고', '같은','것이', '없이','사람들이','얼마', '이번', '회사', '집에', '앞으로', '걸까','아직', '하지', '텐데', '매일', '부모님이', '자신이', '수가', '나에게', '생각이', '않아서', '이번에', '회사에서', '건', '그런', '그래서', '없고', '줄', '그래서', '나만', '없는', '했어', '이다', '하다', '때', '돼', '싶은데', '없어서', '날', '나이가', '같이', '난다', '것', '같아', '나는', '내가', '다', '계속', '나', '내', '더', '남편이', '아내가', '오늘', '난', '정말', '많이', '요즘', '이제', '친구가', '나도', '자꾸', '해', '좀', '나를', '했는데', '응', '그', '갑자기', '거야', '엄마가', '할수', '항상', '잘', '있는데', '어떻게 해야', '때문에', '아들이', '이제는', '친구들이', '딸이', '그렇게', '돈을', '다들', '다시', '않아', '그냥', '있어', '진짜', '돈', '너무', '안', '못', '계', '수', '하고', '없어', '할', '어떻게', '봐', '게', '하는', '우리', '왜', '거', '해서', '한', '있는', '들어', '싶어', '해야', '같아서', '하는데', '일을', '다른', '할지', '걸', '이', '이렇게', '일이', '말을', '것도', '몸이', '나한테', '지금', '있어서', '돈이', '보니', '이런', '사람이'])\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  if isinstance(text, str):\n",
        "    tokens = text.split()\n",
        "    result = [word for word in tokens if not word in stop_words]\n",
        "    return \" \".join(result)\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "emotion['Combined_HS'] = emotion['Combined_HS'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "_62D7Lv3Wh7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all text in the 'Combined_HS' column into a single string\n",
        "text = ' '.join(emotion['Combined_HS'].astype(str).tolist())\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=800, background_color='white', font_path='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf').generate(text)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(8, 8), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7LiJgWGtW0TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df1 = pd.read_csv('/content/labeled_output.csv')"
      ],
      "metadata": {
        "id": "KhDGwPo_120d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.tail(50)"
      ],
      "metadata": {
        "id": "lI54403P1-ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 데이터를 3으로 라벨링을 해버려서 kobert는 파인 튜닝이 필요할 것 같음"
      ],
      "metadata": {
        "id": "kvk8afiUuH8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_data = df1[df1['Emotion_Label'].isnull()] # Use df1 instead of emotional"
      ],
      "metadata": {
        "id": "9AzPQ5I12A0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_data = df1[df1['Emotion_Label'].isnull()]\n",
        "num_nan_data = len(nan_data)\n",
        "\n",
        "print(f\"nan_data는 {num_nan_data}개 있습니다.\")"
      ],
      "metadata": {
        "id": "gKawB3OE6bZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y fonts-nanum"
      ],
      "metadata": {
        "id": "AWUaf3_HQ2JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all text in the 'Combined_HS' column into a single string\n",
        "text = ' '.join(nan_data['Combined_HS'].astype(str).tolist())\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=800, background_color='white', font_path='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf').generate(text)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(8, 8), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KpKDN71oRTUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# 한국어 불용어 추가\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(['맞아','않아','거야','없어','했어','혼자','응','싶어','있어','해','같아','많아서','받아','나의','년','싶지','친구와','하네','시간이','자기','같은데','살','벌써','직장에서','결혼을','돼서','신경','조금','근데','친구들과','어떤','있지','꼭','친구에게','대해','가고','회사에','함께','돈도','나보다','이젠','빨리','친구들은','부모님께','하니','들어서','일도','아주','나랑','결혼','되었어','됐어','노후','모든','학교','할까','나서','몇','건강이','자식들이','하지만','그동안','먼저','학교에서','나와','중이야','많은','병원에','그런데','기분이야','있을','건지','나이','친한','받았어','많아', '뭘','되고','마음이','자주','점점','무슨','위해','있을까','제대로','사는','그래도','일','직장','된','친구를','아무것도','먹고','것을','모두','아무도','하나', '아빠가','참','없는데','말이야','때마다','될','어제','공부를','친구','해도','되는','전에', '하면','보면', '성적이','큰','또', '않고', '같은','것이', '없이','사람들이','얼마', '이번', '회사', '집에', '앞으로', '걸까','아직', '하지', '텐데', '매일', '부모님이', '자신이', '수가', '나에게', '생각이', '않아서', '이번에', '회사에서', '건', '그런', '그래서', '없고', '줄', '그래서', '나만', '없는', '했어', '이다', '하다', '때', '돼', '싶은데', '없어서', '날', '나이가', '같이', '난다', '것', '같아', '나는', '내가', '다', '계속', '나', '내', '더', '남편이', '아내가', '오늘', '난', '정말', '많이', '요즘', '이제', '친구가', '나도', '자꾸', '해', '좀', '나를', '했는데', '응', '그', '갑자기', '거야', '엄마가', '할수', '항상', '잘', '있는데', '어떻게 해야', '때문에', '아들이', '이제는', '친구들이', '딸이', '그렇게', '돈을', '다들', '다시', '않아', '그냥', '있어', '진짜', '돈', '너무', '안', '못', '계', '수', '하고', '없어', '할', '어떻게', '봐', '게', '하는', '우리', '왜', '거', '해서', '한', '있는', '들어', '싶어', '해야', '같아서', '하는데', '일을', '다른', '할지', '걸', '이', '이렇게', '일이', '말을', '것도', '몸이', '나한테', '지금', '있어서', '돈이', '보니', '이런', '사람이'])\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  if isinstance(text, str):\n",
        "    tokens = text.split()\n",
        "    result = [word for word in tokens if not word in stop_words]\n",
        "    return \" \".join(result)\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "nan_data['Combined_HS'] = nan_data['Combined_HS'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "e6qStqiQRkvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "id": "U_mokp0dVsLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Okt 형태소 분석기 초기화\n",
        "okt = Okt()\n",
        "\n",
        "# 한국어 불용어 리스트\n",
        "stop_words = set(['이야기','가족','아이','자식','집','이야','사람','연락','공부','말','애','취업','준비','남편','라고','얘기','로','에서','말','인데','으로','의','까지','생각','들','가','생각','는','도','를','만','에','을','은','맞아','않아','거야','없어','했어','혼자','응','싶어','있어','해','같아','많아서','받아','나의','년','싶지','친구와','하네','시간이','자기','같은데','살','벌써','직장에서','결혼을','돼서','신경','조금','근데','친구들과','어떤','있지','꼭','친구에게','대해','가고','회사에','함께','돈도','나보다','이젠','빨리','친구들은','부모님께','하니','들어서','일도','아주','나랑','결혼','되었어','됐어','노후','모든','학교','할까','나서','몇','건강이','자식들이','하지만','그동안','먼저','학교에서','나와','중이야','많은','병원에','그런데','기분이야','있을','건지','나이','친한','받았어','많아', '뭘','되고','마음이','자주','점점','무슨','위해','있을까','제대로','사는','그래도','일','직장','된','친구를','아무것도','먹고','것을','모두','아무도','하나', '아빠가','참','없는데','말이야','때마다','될','어제','공부를','친구','해도','되는','전에', '하면','보면', '성적이','큰','또', '않고', '같은','것이', '없이','사람들이','얼마', '이번', '회사', '집에', '앞으로', '걸까','아직', '하지', '텐데', '매일', '부모님이', '자신이', '수가', '나에게', '생각이', '않아서', '이번에', '회사에서', '건', '그런', '그래서', '없고', '줄', '그래서', '나만', '없는', '했어', '이다', '하다', '때', '돼', '싶은데', '없어서', '날', '나이가', '같이', '난다', '것', '같아', '나는', '내가', '다', '계속', '나', '내', '더', '남편이', '아내가', '오늘', '난', '정말', '많이', '요즘', '이제', '친구가', '나도', '자꾸', '해', '좀', '나를', '했는데', '응', '그', '갑자기', '거야', '엄마가', '할수', '항상', '잘', '있는데', '어떻게 해야', '때문에', '아들이', '이제는', '친구들이', '딸이', '그렇게', '돈을', '다들', '다시', '않아', '그냥', '있어', '진짜', '돈', '너무', '안', '못', '계', '수', '하고', '없어', '할', '어떻게', '봐', '게', '하는', '우리', '왜', '거', '해서', '한', '있는', '들어', '싶어', '해야', '같아서', '하는데', '일을', '다른', '할지', '걸', '이', '이렇게', '일이', '말을', '것도', '몸이', '나한테', '지금', '있어서', '돈이', '보니', '이런', '사람이'])\n",
        "\n",
        "# 불용어 제거 함수 (Okt 형태소 분석 사용)\n",
        "def remove_stopwords(text):\n",
        "    if isinstance(text, str):\n",
        "        tokens = okt.morphs(text)  # 형태소 분석을 사용한 토큰화\n",
        "        result = [word for word in tokens if word not in stop_words]  # 불용어 제거\n",
        "        return \" \".join(result)\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# 데이터셋에서 Combined_HS 칼럼 불용어 제거 적용\n",
        "nan_data['Combined_HS'] = nan_data['Combined_HS'].apply(remove_stopwords)\n",
        "\n",
        "# 워드 클라우드 생성\n",
        "text = ' '.join(nan_data['Combined_HS'].astype(str).tolist())\n",
        "wordcloud = WordCloud(width=800, height=800, background_color='white', font_path='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf').generate(text)\n",
        "\n",
        "# 워드 클라우드 표시\n",
        "plt.figure(figsize=(8, 8), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "r0NTlsoXRtDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"/content/well_labeling.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "SaYpL0lBSewp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Emotion_Label 값이 없는 행 제거\n",
        "df_cleaned = df.dropna(subset=['Emotion_Label'])\n",
        "\n",
        "# 결과 확인\n",
        "print(df_cleaned)"
      ],
      "metadata": {
        "id": "-KJ_-NP3jKYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch"
      ],
      "metadata": {
        "id": "emZL8N98kolb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라벨 값이 0부터 시작하도록 조정\n",
        "# .loc를 사용하여 데이터프레임의 컬럼 값을 수정\n",
        "df_cleaned = df_cleaned.sort_values(by='Emotion_Label').reset_index(drop=True)"
      ],
      "metadata": {
        "id": "DqxBRpb-r6NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.head(20)"
      ],
      "metadata": {
        "id": "qvqw89WOsBpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, AutoTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# KoBERT 토크나이저 로드\n",
        "tokenizer = AutoTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "\n",
        "# 라벨 및 텍스트 리스트 생성\n",
        "texts = df_cleaned['Combined_HS'].tolist()\n",
        "labels = df_cleaned['Emotion_Label'].tolist()\n",
        "\n",
        "# 데이터셋 생성\n",
        "class KoBERTDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # 텍스트를 토큰화하여 텐서로 변환\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)  # 라벨을 long 타입으로 변환\n",
        "        }\n",
        "\n",
        "# 데이터셋 분리\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_dataset = KoBERTDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = KoBERTDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "\n",
        "# 모델 초기화\n",
        "model = BertForSequenceClassification.from_pretrained('skt/kobert-base-v1', num_labels=10)"
      ],
      "metadata": {
        "id": "0_E-_nRflDDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "import torch\n",
        "\n",
        "# 옵티마이저 설정\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# 학습 함수\n",
        "def train_epoch(model, data_loader, optimizer, device):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return sum(losses) / len(losses)\n",
        "\n",
        "# 검증 함수\n",
        "def eval_model(model, data_loader, device):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return sum(losses) / len(losses)\n"
      ],
      "metadata": {
        "id": "orUHKA4oo92y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라벨을 정수형으로 변환하고 값의 범위 확인\n",
        "df_cleaned['Emotion_Label'] = df_cleaned['Emotion_Label'].astype(int)\n",
        "print(f\"라벨 값의 범위: 최소={df_cleaned['Emotion_Label'].min()}, 최대={df_cleaned['Emotion_Label'].max()}\")"
      ],
      "metadata": {
        "id": "5Yr7ap-SpUt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_label = df_cleaned['Emotion_Label'].min()\n",
        "max_label = df_cleaned['Emotion_Label'].max()\n",
        "print(f\"라벨 값의 범위: 최소={min_label}, 최대={max_label}\")"
      ],
      "metadata": {
        "id": "RHfFU1pkqv4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1"
      ],
      "metadata": {
        "id": "l9JQPw1wtuPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디바이스 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"gpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# 학습 에포크 수\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "    val_loss = eval_model(model, val_loader, device)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss} | Validation Loss: {val_loss}\")"
      ],
      "metadata": {
        "id": "cwOAJhbXo_pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 함수\n",
        "def train_epoch(model, data_loader, optimizer, device):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return sum(losses) / len(losses)\n",
        "\n",
        "# 검증 함수\n",
        "def eval_model(model, data_loader, device):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return sum(losses) / len(losses)"
      ],
      "metadata": {
        "id": "f0NJAAkxlcKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q1asce0MlkEJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}