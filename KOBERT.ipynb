{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPaEjqFKdJAI83c3s1caNIh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tlsgptj/emotional_BERT/blob/main/KOBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#크롤링\n",
        "#기쁨, 슬픔, 놀람, 분노, 공포, 혐오\n",
        "import json\n",
        "import csv\n",
        "\n",
        "with open('/content/감성대화말뭉치(최종데이터)_Training.json', 'r') as json_file:\n",
        "  data = json.load(json_file)\n",
        "\n",
        "with open('csv_file.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
        "  writer = csv.writer(csv_file)\n",
        "  header = data[0].keys()\n",
        "  writer.writerow(header)\n",
        "  for row in data:\n",
        "    writer.writerow(row.values())"
      ],
      "metadata": {
        "id": "Zx68RIw0yHNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mxnet # 코랩 환경이기 때문에 앞에 !를 붙여야 한다.\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "I3XjGr26nOZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, BertModel\n",
        "\n",
        "# Try loading the tokenizer using AutoTokenizer\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    print(\"Please check the model name and your internet connection.\")\n",
        "\n",
        "# Load the KoBERT model\n",
        "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "\n",
        "# ... rest of your code ...le_with_warmup"
      ],
      "metadata": {
        "id": "lnXM2h4HoOto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "# JSON 파일 로드\n",
        "with open('/content/감성대화말뭉치(최종데이터)_Training.json', 'r', encoding='utf-8') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "# HS01과 HS02 추출 및 합치기\n",
        "extracted_data = []\n",
        "for row in data:\n",
        "    talk_data = row.get('talk', {})  # 'talk' 데이터를 가져옴\n",
        "    content = talk_data.get('content', {})\n",
        "    hs01 = content.get('HS01', '')\n",
        "    hs02 = content.get('HS02', '')\n",
        "\n",
        "    # HS01과 HS02를 합치기\n",
        "    combined_hs = f\"{hs01} {hs02}\".strip()  # 문자열을 합치고 불필요한 공백 제거\n",
        "\n",
        "    if combined_hs:  # combined_hs가 비어있지 않으면 추가\n",
        "        extracted_data.append([combined_hs])\n",
        "\n",
        "# CSV 파일로 저장\n",
        "csv_file_path = 'extracted_hs_data.csv'\n",
        "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Combined_HS'])  # 컬럼명 작성\n",
        "    writer.writerows(extracted_data)\n",
        "\n",
        "print(f'CSV 파일이 {csv_file_path}에 저장되었습니다.')"
      ],
      "metadata": {
        "id": "QLnFCdATpbUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# talk에 있는 데이터만 추출해야함"
      ],
      "metadata": {
        "id": "3tXftmuxwUHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "emotional = pd.read_csv('/content/extracted_hs_data.csv')"
      ],
      "metadata": {
        "id": "02k08tMEw3z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotional.head(20)"
      ],
      "metadata": {
        "id": "jzBRgkH1y0RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, BertModel\n",
        "\n",
        "# Try loading the tokenizer using AutoTokenizer\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")\n",
        "    print(\"Please check the model name and your internet connection.\")\n",
        "\n",
        "# Load the KoBERT model\n",
        "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "\n",
        "# ... rest of your code ...le_with_warmup"
      ],
      "metadata": {
        "id": "Oz3PonV5zQPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 과정\n",
        "# 1. 텍스트 데이터 정제\n",
        "# 2. 문서를 토큰으로 나누기\n",
        "# 3. 불용어 제거\n",
        "# 4. 모델을 적용할꺼임 KOBERT적용 등...\n",
        "# 5. TF-DIF로 알고리즘 비교\n",
        "# koBERT의 경우 벡터로 변환할 필요가 없다고 함 : KoBERT는 텍스트를 바로 토큰화한 뒤 임베딩 벡터로 변환하는 기능을 내장하고 있기 때문"
      ],
      "metadata": {
        "id": "DvVkTo8Bzqq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 과정 자세히\n",
        "# [영어]\n",
        "# 소문자 변환\n",
        "# 불필요한 공백 제거\n",
        "# 구두점 제거\n",
        "# 특수 문자 제거\n",
        "# 숫자 제거\n",
        "# 불용어 제거\n",
        "# 형태소 분석\n",
        "# 어간 추출\n",
        "# 표제어 추출\n",
        "# 철자 교정\n",
        "# 단어 토큰화\n",
        "# 벡터화\n",
        "## -> 여기서 한국어일 경우 1번은 생략하고 분석"
      ],
      "metadata": {
        "id": "XP0oE22I0NGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 구두점 제거\n",
        "import string\n",
        "import re\n",
        "emotional['Combined_HS'] = emotional['Combined_HS'].str.translate(str.maketrans('', '', string.punctuation))\n",
        "# 특수 문자 제거\n",
        "emotional['Combined_HS'] = emotional['Combined_HS'].apply(lambda x: re.sub(r\"[^a-zA-Z0-9가-힣\\s]\", \"\", x) if isinstance(x, str) else x)\n",
        "# 숫자 제거\n",
        "emotional['Combined_HS'] = emotional['Combined_HS'].apply(lambda x: re.sub(r'\\d+', '', x) if isinstance(x, str) else x)"
      ],
      "metadata": {
        "id": "dIHR3oZO0bjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# 한국어 불용어 추가\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(['많아서','받아','나의','년','싶지','친구와','하네','시간이','자기','같은데','살','벌써','직장에서','결혼을','돼서','신경','조금','근데','친구들과','어떤','있지','꼭','친구에게','대해','가고','회사에','함께','돈도','나보다','이젠','빨리','친구들은','부모님께','하니','들어서','일도','아주','나랑','결혼','되었어','됐어','노후','모든','학교','할까','나서','몇','건강이','자식들이','하지만','그동안','먼저','학교에서','나와','중이야','많은','병원에','그런데','기분이야','있을','건지','나이','친한','받았어','많아', '뭘','되고','마음이','자주','점점','무슨','위해','있을까','제대로','사는','그래도','일','직장','된','친구를','아무것도','먹고','것을','모두','아무도','하나', '아빠가','참','없는데','말이야','때마다','될','어제','공부를','친구','해도','되는','전에', '하면','보면', '성적이','큰','또', '않고', '같은','것이', '없이','사람들이','얼마', '이번', '회사', '집에', '앞으로', '걸까','아직', '하지', '텐데', '매일', '부모님이', '자신이', '수가', '나에게', '생각이', '않아서', '이번에', '회사에서', '건', '그런', '그래서', '없고', '줄', '그래서', '나만', '없는', '했어', '이다', '하다', '때', '돼', '싶은데', '없어서', '날', '나이가', '같이', '난다', '것', '같아', '나는', '내가', '다', '계속', '나', '내', '더', '남편이', '아내가', '오늘', '난', '정말', '많이', '요즘', '이제', '친구가', '나도', '자꾸', '해', '좀', '나를', '했는데', '응', '그', '갑자기', '거야', '엄마가', '할수', '항상', '잘', '있는데', '어떻게 해야', '때문에', '아들이', '이제는', '친구들이', '딸이', '그렇게', '돈을', '다들', '다시', '않아', '그냥', '있어', '진짜', '돈', '너무', '안', '못', '계', '수', '하고', '없어', '할', '어떻게', '봐', '게', '하는', '우리', '왜', '거', '해서', '한', '있는', '들어', '싶어', '해야', '같아서', '하는데', '일을', '다른', '할지', '걸', '이', '이렇게', '일이', '말을', '것도', '몸이', '나한테', '지금', '있어서', '돈이', '보니', '이런', '사람이'])\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  if isinstance(text, str):\n",
        "    tokens = text.split()\n",
        "    result = [word for word in tokens if not word in stop_words]\n",
        "    return \" \".join(result)\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "emotional['Combined_HS'] = emotional['Combined_HS'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "vK4K6qF66bY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y fonts-nanum"
      ],
      "metadata": {
        "id": "TPFtVoRhOqkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all text in the 'Combined_HS' column into a single string\n",
        "text = ' '.join(emotional['Combined_HS'].astype(str).tolist())\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=800, background_color='white', font_path='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf').generate(text)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(8, 8), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0dbCUkChOdU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotional.head(20)"
      ],
      "metadata": {
        "id": "OOJ9nGFr0iKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "emotion_words = {\n",
        "    \"기쁨\": [\"지\",\"만족\", \"자랑\", \"어\", \"좋\", \"좋아\", \"기분\", \"다행\", \"원하\", \"웃\", \"성공\", \"합격\", \"가벼워\", \"행복\", \"편해\", \"은\", \"하는\", \"기쁘\", \"다\"],\n",
        "    \"슬픔\": [\"억울\",\"해\",\"속상\",\"슬프\",\"어\",\"눈물이\",\"아파\",\"해\", \"속상\",\"해\", \"받\", \"상처\", \"원망\", \"스러워\", \"안쓰럽\", \"낙담\", \"슬프\", \"힘들\", \"미안\", \"후회\", \"억울\", \"괴로워\", \"외롭\", \"한심\", \"눈물\", \"섭섭\", \"취소\", \"버리\", \"망하\", \"실망\", \"싸우\", \"불쌍\", \"서운\", \"미워\", \"서글프\", \"슬프\", \"씁쓸\", \"서러\"],\n",
        "    \"걱정\": [\"무서워\",\"불안\",\"해\",\"당황\", \"혼란\", \"두렵\", \"모르\", \"초조\", \"걱정\", \"어떡하\", \"불안\", \"고민\", \"마비\", \"하다\"],\n",
        "    \"분노\": [\"가\",\"역겨\", \"환멸\", \"원망\", \"얄밉\", \"짜증\", \"화\", \"스트레스\", \"답답\", \"분하\", \"배신\", \"버리\", \"의심\"],\n",
        "    \"공포\": [\"모르겠어\",\"무섭\", \"두렵\", \"두려움\", \"무서움\", \"힘들\"],\n",
        "    \"우울\": [\"힘들어\",\"힘든\",\"힘드네\",\"힘들\", \"외롭\", \"어렵\", \"버겁\", \"우울\",\"해\", \"고독\", \"쓸쓸\", \"하네\", \"하다\"],\n",
        "    \"부끄러움\": [\"당황\", \"창피\", \"부끄럽\", \"실수\", \"당혹\", \"민망\", \"망신\"],\n",
        "    \"감사\": [\"감사\"],\n",
        "    \"놀람\": [\"놀랐\"],\n",
        "    \"후회\": [\"후회\", \"돼\", \"스러워\"],\n",
        "    \"기타\": []\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# 감정 레이블\n",
        "emotion_labels = {\n",
        "    \"기쁨\": 0,\n",
        "    \"슬픔\": 1,\n",
        "    \"걱정\": 2,\n",
        "    \"분노\": 3,\n",
        "    \"공포\": 4,\n",
        "    \"우울\": 5,\n",
        "    \"부끄러움\": 6,\n",
        "    \"감사\":7,\n",
        "    \"놀람\":8,\n",
        "    \"후회\":9,\n",
        "    \"기타\":10\n",
        "}\n",
        "\n",
        "# 텍스트에서 문장 부호 제거 함수\n",
        "def clean_text(text):\n",
        "    # 정규 표현식을 사용하여 문자와 숫자가 아닌 것을 공백으로 치환\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# 텍스트에서 감정 레이블 추출 함수 (형태소 분석 추가)\n",
        "def label_emotion(text):\n",
        "    text = clean_text(text)  # 문장 부호 제거\n",
        "    tokens = okt.morphs(text)  # 형태소 분석을 통해 텍스트를 토큰화\n",
        "    emotion_count = {emotion: 0 for emotion in emotion_words}  # 감정별 카운트를 위한 딕셔너리\n",
        "\n",
        "    for emotion, words in emotion_words.items():\n",
        "        for word in words:\n",
        "            if word in tokens:\n",
        "                emotion_count[emotion] += 1  # 감정 단어가 발견되면 카운트 증가\n",
        "\n",
        "    max_emotion = max(emotion_count, key=emotion_count.get)  # 가장 많이 등장한 감정 선택\n",
        "    if emotion_count[max_emotion] > 0:\n",
        "        return emotion_labels[max_emotion]\n",
        "    return emotion_labels[\"기타\"]  # 감정 단어가 없을 경우 \"기타\"로 처리\n",
        "\n",
        "# CSV 파일에서 텍스트 읽고 라벨링한 후 저장[[/0]]\n",
        "def label_csv(input_file, output_file):\n",
        "    with open(input_file, mode='r', encoding='utf-8') as infile, open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
        "        reader = csv.reader(infile)\n",
        "        writer = csv.writer(outfile)\n",
        "\n",
        "        # 헤더 작성\n",
        "        header = next(reader)  # 기존 CSV의 헤더를 가져옴\n",
        "        header.append('Emotion_Label')  # 새로운 열 추가\n",
        "        writer.writerow(header)\n",
        "\n",
        "        # 각 행에 대해 감정 레이블 추가\n",
        "        for row in reader:\n",
        "            text = row[0]  # 텍스트가 들어 있는 열 (필요 시 인덱스 조정)\n",
        "            emotion_label = label_emotion(text)\n",
        "            row.append(emotion_label)\n",
        "            writer.writerow(row)\n",
        "\n",
        "# 10번 라벨(기타)만 추출하는 함수\n",
        "def extract_label_10(input_file, output_file):\n",
        "    with open(input_file, mode='r', encoding='utf-8') as infile, open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
        "        reader = csv.reader(infile)\n",
        "        writer = csv.writer(outfile)\n",
        "\n",
        "        # 헤더 작성\n",
        "        header = next(reader)\n",
        "        writer.writerow(header)\n",
        "\n",
        "        # 10번 레이블(기타)인 행만 추출하여 작성\n",
        "        for row in reader:\n",
        "            if row[-1] == '10':  # 마지막 열이 '10'(기타 레이블)인 경우\n",
        "                writer.writerow(row)\n",
        "\n",
        "# 10번 라벨(기타)만 추출하고 개수 카운트하는 함수\n",
        "def extract_label_10(input_file, output_file):\n",
        "    count = 0  # 10번 라벨의 개수를 세기 위한 변수\n",
        "    with open(input_file, mode='r', encoding='utf-8') as infile, open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
        "        reader = csv.reader(infile)\n",
        "        writer = csv.writer(outfile)\n",
        "\n",
        "        # 헤더 작성\n",
        "        header = next(reader)\n",
        "        writer.writerow(header)\n",
        "\n",
        "        # 10번 레이블(기타)인 행만 추출하여 작성\n",
        "        for row in reader:\n",
        "            if row[-1] == '10':  # 마지막 열이 '10'(기타 레이블)인 경우\n",
        "                writer.writerow(row)\n",
        "                count += 1  # 10번 레이블 개수 증가\n",
        "\n",
        "    print(f\"10번 라벨(기타) 데이터는 총 {count}개 있습니다.\")\n",
        "    return count\n",
        "\n",
        "# 라벨 10번만 추출하여 저장할 파일 경로\n",
        "label_10_output = 'label_10_output.csv'\n",
        "count_10 = extract_label_10(output_csv, label_10_output)\n",
        "\n",
        "print(f\"라벨 10번 데이터가 '{label_10_output}'에 저장되었으며, 총 {count_10}개의 행이 10번 라벨로 분류되었습니다.\")\n",
        "\n",
        "# CSV 파일 경로 설정\n",
        "input_csv = '/content/extracted_hs_data.csv'  # 감정 분석할 CSV 파일 경로\n",
        "output_csv = 'labeled_output.csv'  # 라벨링된 결과를 저장할 CSV 파일 경로\n",
        "\n",
        "# CSV 파일에 라벨링 적용\n",
        "label_csv(input_csv, output_csv)\n"
      ],
      "metadata": {
        "id": "yBkSViTuvya1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y fonts-nanum"
      ],
      "metadata": {
        "id": "-95D08ScPOCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "emotion = pd.read_csv('/content/label_10_output.csv')"
      ],
      "metadata": {
        "id": "LT002IJTPDFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion.tail(20)"
      ],
      "metadata": {
        "id": "j6kasB89lXPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 구두점 제거\n",
        "import string\n",
        "import re\n",
        "emotion['Combined_HS'] = emotion['Combined_HS'].str.translate(str.maketrans('', '', string.punctuation))\n",
        "# 특수 문자 제거\n",
        "emotion['Combined_HS'] = emotion['Combined_HS'].apply(lambda x: re.sub(r\"[^a-zA-Z0-9가-힣\\s]\", \"\", x) if isinstance(x, str) else x)\n",
        "# 숫자 제거\n",
        "emotion['Combined_HS'] = emotion['Combined_HS'].apply(lambda x: re.sub(r'\\d+', '', x) if isinstance(x, str) else x)"
      ],
      "metadata": {
        "id": "JP1CC3d_WYbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# 한국어 불용어 추가\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(['아무래도','그래','있었어','어려워','기분이','느껴져','고민이야','친구는','있었는데','좋겠어','죄책감이','싶은','보고','싫어','맞아','혼자','열심히','많아서','받아','나의','년','싶지','친구와','하네','시간이','자기','같은데','살','벌써','직장에서','결혼을','돼서','신경','조금','근데','친구들과','어떤','있지','꼭','친구에게','대해','가고','회사에','함께','돈도','나보다','이젠','빨리','친구들은','부모님께','하니','들어서','일도','아주','나랑','결혼','되었어','됐어','노후','모든','학교','할까','나서','몇','건강이','자식들이','하지만','그동안','먼저','학교에서','나와','중이야','많은','병원에','그런데','기분이야','있을','건지','나이','친한','받았어','많아', '뭘','되고','마음이','자주','점점','무슨','위해','있을까','제대로','사는','그래도','일','직장','된','친구를','아무것도','먹고','것을','모두','아무도','하나', '아빠가','참','없는데','말이야','때마다','될','어제','공부를','친구','해도','되는','전에', '하면','보면', '성적이','큰','또', '않고', '같은','것이', '없이','사람들이','얼마', '이번', '회사', '집에', '앞으로', '걸까','아직', '하지', '텐데', '매일', '부모님이', '자신이', '수가', '나에게', '생각이', '않아서', '이번에', '회사에서', '건', '그런', '그래서', '없고', '줄', '그래서', '나만', '없는', '했어', '이다', '하다', '때', '돼', '싶은데', '없어서', '날', '나이가', '같이', '난다', '것', '같아', '나는', '내가', '다', '계속', '나', '내', '더', '남편이', '아내가', '오늘', '난', '정말', '많이', '요즘', '이제', '친구가', '나도', '자꾸', '해', '좀', '나를', '했는데', '응', '그', '갑자기', '거야', '엄마가', '할수', '항상', '잘', '있는데', '어떻게 해야', '때문에', '아들이', '이제는', '친구들이', '딸이', '그렇게', '돈을', '다들', '다시', '않아', '그냥', '있어', '진짜', '돈', '너무', '안', '못', '계', '수', '하고', '없어', '할', '어떻게', '봐', '게', '하는', '우리', '왜', '거', '해서', '한', '있는', '들어', '싶어', '해야', '같아서', '하는데', '일을', '다른', '할지', '걸', '이', '이렇게', '일이', '말을', '것도', '몸이', '나한테', '지금', '있어서', '돈이', '보니', '이런', '사람이'])\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  if isinstance(text, str):\n",
        "    tokens = text.split()\n",
        "    result = [word for word in tokens if not word in stop_words]\n",
        "    return \" \".join(result)\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "emotion['Combined_HS'] = emotion['Combined_HS'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "_62D7Lv3Wh7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all text in the 'Combined_HS' column into a single string\n",
        "text = ' '.join(emotion['Combined_HS'].astype(str).tolist())\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=800, background_color='white', font_path='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf').generate(text)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(8, 8), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7LiJgWGtW0TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df1 = pd.read_csv('/content/labeled_output.csv')"
      ],
      "metadata": {
        "id": "KhDGwPo_120d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.tail(50)"
      ],
      "metadata": {
        "id": "lI54403P1-ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 데이터를 3으로 라벨링을 해버려서 kobert는 파인 튜닝이 필요할 것 같음"
      ],
      "metadata": {
        "id": "kvk8afiUuH8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_data = df1[df1['Emotion_Label'].isnull()] # Use df1 instead of emotional"
      ],
      "metadata": {
        "id": "9AzPQ5I12A0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_data = df1[df1['Emotion_Label'].isnull()]\n",
        "num_nan_data = len(nan_data)\n",
        "\n",
        "print(f\"nan_data는 {num_nan_data}개 있습니다.\")"
      ],
      "metadata": {
        "id": "gKawB3OE6bZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y fonts-nanum"
      ],
      "metadata": {
        "id": "AWUaf3_HQ2JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all text in the 'Combined_HS' column into a single string\n",
        "text = ' '.join(nan_data['Combined_HS'].astype(str).tolist())\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=800, background_color='white', font_path='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf').generate(text)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(8, 8), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KpKDN71oRTUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# 한국어 불용어 추가\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(['맞아','않아','거야','없어','했어','혼자','응','싶어','있어','해','같아','많아서','받아','나의','년','싶지','친구와','하네','시간이','자기','같은데','살','벌써','직장에서','결혼을','돼서','신경','조금','근데','친구들과','어떤','있지','꼭','친구에게','대해','가고','회사에','함께','돈도','나보다','이젠','빨리','친구들은','부모님께','하니','들어서','일도','아주','나랑','결혼','되었어','됐어','노후','모든','학교','할까','나서','몇','건강이','자식들이','하지만','그동안','먼저','학교에서','나와','중이야','많은','병원에','그런데','기분이야','있을','건지','나이','친한','받았어','많아', '뭘','되고','마음이','자주','점점','무슨','위해','있을까','제대로','사는','그래도','일','직장','된','친구를','아무것도','먹고','것을','모두','아무도','하나', '아빠가','참','없는데','말이야','때마다','될','어제','공부를','친구','해도','되는','전에', '하면','보면', '성적이','큰','또', '않고', '같은','것이', '없이','사람들이','얼마', '이번', '회사', '집에', '앞으로', '걸까','아직', '하지', '텐데', '매일', '부모님이', '자신이', '수가', '나에게', '생각이', '않아서', '이번에', '회사에서', '건', '그런', '그래서', '없고', '줄', '그래서', '나만', '없는', '했어', '이다', '하다', '때', '돼', '싶은데', '없어서', '날', '나이가', '같이', '난다', '것', '같아', '나는', '내가', '다', '계속', '나', '내', '더', '남편이', '아내가', '오늘', '난', '정말', '많이', '요즘', '이제', '친구가', '나도', '자꾸', '해', '좀', '나를', '했는데', '응', '그', '갑자기', '거야', '엄마가', '할수', '항상', '잘', '있는데', '어떻게 해야', '때문에', '아들이', '이제는', '친구들이', '딸이', '그렇게', '돈을', '다들', '다시', '않아', '그냥', '있어', '진짜', '돈', '너무', '안', '못', '계', '수', '하고', '없어', '할', '어떻게', '봐', '게', '하는', '우리', '왜', '거', '해서', '한', '있는', '들어', '싶어', '해야', '같아서', '하는데', '일을', '다른', '할지', '걸', '이', '이렇게', '일이', '말을', '것도', '몸이', '나한테', '지금', '있어서', '돈이', '보니', '이런', '사람이'])\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  if isinstance(text, str):\n",
        "    tokens = text.split()\n",
        "    result = [word for word in tokens if not word in stop_words]\n",
        "    return \" \".join(result)\n",
        "  else:\n",
        "    return text\n",
        "\n",
        "nan_data['Combined_HS'] = nan_data['Combined_HS'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "e6qStqiQRkvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "id": "U_mokp0dVsLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Okt 형태소 분석기 초기화\n",
        "okt = Okt()\n",
        "\n",
        "# 한국어 불용어 리스트\n",
        "stop_words = set(['이야기','가족','아이','자식','집','이야','사람','연락','공부','말','애','취업','준비','남편','라고','얘기','로','에서','말','인데','으로','의','까지','생각','들','가','생각','는','도','를','만','에','을','은','맞아','않아','거야','없어','했어','혼자','응','싶어','있어','해','같아','많아서','받아','나의','년','싶지','친구와','하네','시간이','자기','같은데','살','벌써','직장에서','결혼을','돼서','신경','조금','근데','친구들과','어떤','있지','꼭','친구에게','대해','가고','회사에','함께','돈도','나보다','이젠','빨리','친구들은','부모님께','하니','들어서','일도','아주','나랑','결혼','되었어','됐어','노후','모든','학교','할까','나서','몇','건강이','자식들이','하지만','그동안','먼저','학교에서','나와','중이야','많은','병원에','그런데','기분이야','있을','건지','나이','친한','받았어','많아', '뭘','되고','마음이','자주','점점','무슨','위해','있을까','제대로','사는','그래도','일','직장','된','친구를','아무것도','먹고','것을','모두','아무도','하나', '아빠가','참','없는데','말이야','때마다','될','어제','공부를','친구','해도','되는','전에', '하면','보면', '성적이','큰','또', '않고', '같은','것이', '없이','사람들이','얼마', '이번', '회사', '집에', '앞으로', '걸까','아직', '하지', '텐데', '매일', '부모님이', '자신이', '수가', '나에게', '생각이', '않아서', '이번에', '회사에서', '건', '그런', '그래서', '없고', '줄', '그래서', '나만', '없는', '했어', '이다', '하다', '때', '돼', '싶은데', '없어서', '날', '나이가', '같이', '난다', '것', '같아', '나는', '내가', '다', '계속', '나', '내', '더', '남편이', '아내가', '오늘', '난', '정말', '많이', '요즘', '이제', '친구가', '나도', '자꾸', '해', '좀', '나를', '했는데', '응', '그', '갑자기', '거야', '엄마가', '할수', '항상', '잘', '있는데', '어떻게 해야', '때문에', '아들이', '이제는', '친구들이', '딸이', '그렇게', '돈을', '다들', '다시', '않아', '그냥', '있어', '진짜', '돈', '너무', '안', '못', '계', '수', '하고', '없어', '할', '어떻게', '봐', '게', '하는', '우리', '왜', '거', '해서', '한', '있는', '들어', '싶어', '해야', '같아서', '하는데', '일을', '다른', '할지', '걸', '이', '이렇게', '일이', '말을', '것도', '몸이', '나한테', '지금', '있어서', '돈이', '보니', '이런', '사람이'])\n",
        "\n",
        "# 불용어 제거 함수 (Okt 형태소 분석 사용)\n",
        "def remove_stopwords(text):\n",
        "    if isinstance(text, str):\n",
        "        tokens = okt.morphs(text)  # 형태소 분석을 사용한 토큰화\n",
        "        result = [word for word in tokens if word not in stop_words]  # 불용어 제거\n",
        "        return \" \".join(result)\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# 데이터셋에서 Combined_HS 칼럼 불용어 제거 적용\n",
        "nan_data['Combined_HS'] = nan_data['Combined_HS'].apply(remove_stopwords)\n",
        "\n",
        "# 워드 클라우드 생성\n",
        "text = ' '.join(nan_data['Combined_HS'].astype(str).tolist())\n",
        "wordcloud = WordCloud(width=800, height=800, background_color='white', font_path='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf').generate(text)\n",
        "\n",
        "# 워드 클라우드 표시\n",
        "plt.figure(figsize=(8, 8), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "r0NTlsoXRtDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SaYpL0lBSewp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}